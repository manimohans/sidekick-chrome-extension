# Chrome Web Store Listing

## Short Description (132 characters max)
Chat with your local LLM right from your browser. Works with Ollama, LM Studio, and any OpenAI-compatible server.

## Long Description (16000 characters max)

Sidekick brings the power of local AI directly into your browser. Chat with your own locally-running language models without sending any data to the cloud. Your conversations stay on your machine, giving you complete privacy and control.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ¤– WHAT IS SIDEKICK?

Sidekick is a browser sidebar that connects to your local LLM server. Think of it as having ChatGPT-like capabilities, but running entirely on your own hardware. No API keys needed. No subscription fees. No data leaving your computer.

Whether you're summarizing articles, drafting professional emails, extracting action items from meeting notes, or just having a conversation - Sidekick makes it easy to leverage AI while browsing.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âœ¨ KEY FEATURES

ã€Sidebar Interfaceã€‘
Sidekick opens as a convenient sidebar panel, so you can chat with AI while viewing any webpage. No popups or new tabs - everything stays in one place.

ã€Text Selection Integrationã€‘
Select any text on a webpage and Sidekick automatically detects it. Perfect for asking questions about content, getting summaries, or transforming text.

ã€Quick Actionsã€‘
Use the dropdown menu or slash commands to quickly:
â€¢ /summarize - Get concise summaries of long content
â€¢ /explain - Break down complex topics in simple terms
â€¢ /professional - Rewrite text in a professional tone
â€¢ /actions - Extract action items and to-dos
â€¢ /twitter - Convert content into a tweet thread

ã€Slash Command Autocompleteã€‘
Type "/" to see all available commands with descriptions. Use arrow keys to navigate and Tab to complete - just like a modern code editor.

ã€Streaming Responsesã€‘
See responses as they're generated in real-time, just like ChatGPT. No waiting for the complete response before seeing any output.

ã€Conversation Historyã€‘
Sidekick remembers your conversation context, allowing for natural back-and-forth dialogue. Clear the chat anytime to start fresh.

ã€Stop Generationã€‘
Changed your mind? Click the stop button to halt response generation at any time.

ã€Markdown Supportã€‘
Responses are beautifully formatted with full markdown support - headings, bold, italics, code blocks, lists, and more.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ”’ PRIVACY FIRST

Unlike cloud-based AI services, Sidekick keeps everything local:

â€¢ Your prompts never leave your computer
â€¢ Your conversations are not stored on any server
â€¢ No account required
â€¢ No API keys to manage
â€¢ No usage limits or quotas
â€¢ No monthly fees

This makes Sidekick perfect for:
â€¢ Working with sensitive or confidential information
â€¢ Corporate environments with strict data policies
â€¢ Users who value privacy and data ownership
â€¢ Anyone who wants AI assistance without the cloud

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ–¥ï¸ SUPPORTED LLM SERVERS

Sidekick works with any server that supports the OpenAI-compatible API format (/v1/chat/completions endpoint):

ã€Ollamaã€‘
The most popular way to run LLMs locally. Free and open source.
â€¢ Default address: http://localhost:11434
â€¢ Install from: https://ollama.ai
â€¢ Run: ollama pull gemma3:1b

ã€LM Studioã€‘
User-friendly desktop app for running local LLMs with a nice GUI.
â€¢ Default address: http://localhost:1234
â€¢ Enable "Local Server" in settings
â€¢ Download from: https://lmstudio.ai

ã€llama.cppã€‘
Lightweight, high-performance LLM inference.
â€¢ Run with the --api flag
â€¢ Great for advanced users

ã€vLLMã€‘
High-throughput LLM serving.
â€¢ Default address: http://localhost:8000
â€¢ Excellent for multi-user setups

ã€Text Generation WebUI (Oobabooga)ã€‘
Feature-rich web interface for LLMs.
â€¢ Enable the API extension

ã€AnythingLLMã€‘
All-in-one AI application.
â€¢ Check server settings for API address

ã€Any OpenAI-Compatible Serverã€‘
If it supports /v1/chat/completions, it works with Sidekick!

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“‹ QUICK START GUIDE

1. Install a Local LLM Server
   The easiest option is Ollama:
   â€¢ Download from https://ollama.ai
   â€¢ Install and run it
   â€¢ Open terminal and run: ollama pull gemma3:1b

2. Configure Sidekick
   â€¢ Click the Sidekick icon in your browser toolbar
   â€¢ Click the settings gear icon
   â€¢ Enter your server address (e.g., http://localhost:11434)
   â€¢ Enter your model name (e.g., gemma3:1b or google/gemma-3-1b)
   â€¢ Click "Test Connection" to verify

3. Start Chatting!
   â€¢ Click the Sidekick icon to open the sidebar
   â€¢ Type a message or select text on any page
   â€¢ Press Enter or click Send

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ’¡ USE CASES

ã€Research & Learningã€‘
â€¢ Summarize long articles and papers
â€¢ Get explanations of complex topics
â€¢ Ask follow-up questions about content

ã€Writing & Communicationã€‘
â€¢ Draft professional emails
â€¢ Improve the tone of your writing
â€¢ Generate tweet threads from articles

ã€Productivityã€‘
â€¢ Extract action items from meeting notes
â€¢ Create to-do lists from documents
â€¢ Quickly understand lengthy documents

ã€Developmentã€‘
â€¢ Explain code snippets
â€¢ Get coding help while browsing documentation
â€¢ Debug errors by pasting stack traces

ã€Content Creationã€‘
â€¢ Generate ideas and outlines
â€¢ Rewrite content for different audiences
â€¢ Transform content between formats

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âŒ¨ï¸ SLASH COMMANDS REFERENCE

Type these at the start of your message:

/summarize - Condense content into key points
/summary - Same as /summarize
/explain - Explain in simple, clear terms
/eli5 - Same as /explain (Explain Like I'm 5)
/professional - Rewrite in professional tone
/pro - Same as /professional
/actions - Extract action items and tasks
/todos - Same as /actions
/twitter - Convert to tweet thread format
/thread - Same as /twitter
/tweet - Same as /twitter
/chat - Regular conversation (default)

Pro tip: Type "/" to see the autocomplete menu!

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ”§ TROUBLESHOOTING

ã€Connection Issuesã€‘
â€¢ Make sure your LLM server is running
â€¢ Check that the server address is correct
â€¢ Try http://localhost:11434 for Ollama
â€¢ For remote servers, ensure the port is accessible

ã€"Server responded with 400"ã€‘
â€¢ Check that the model name is correct
â€¢ For LM Studio, use the full model ID (e.g., google/gemma-3-1b)
â€¢ Make sure there's no trailing slash in the server URL

ã€"Server responded with 404"ã€‘
â€¢ Your server might not support the OpenAI API format
â€¢ For Ollama, make sure you're using a recent version
â€¢ Check that the API endpoint is enabled

ã€Slow Responsesã€‘
â€¢ Try a smaller model (e.g., gemma3:1b instead of larger models)
â€¢ Use quantized models for faster inference
â€¢ Ensure your system has enough RAM for the model

ã€Remote Server Setupã€‘
â€¢ Use the server's IP address (e.g., http://192.168.1.100:11434)
â€¢ For Ollama, set OLLAMA_HOST=0.0.0.0 on the server
â€¢ Make sure the firewall allows the port

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸš€ RECOMMENDED MODELS

For the best experience, we recommend these models:

ã€Small & Fastã€‘
â€¢ gemma3:1b - Great balance of speed and quality
â€¢ phi3 - Microsoft's compact but capable model
â€¢ tinyllama - Ultra-lightweight option

ã€Medium & Capableã€‘
â€¢ gemma3:4b - Better quality, still fast
â€¢ llama3:8b - Meta's excellent general-purpose model
â€¢ mistral - Strong all-around performer

ã€Large & Powerfulã€‘
â€¢ gemma3:27b - Near-frontier quality
â€¢ llama3:70b - Top-tier performance (requires significant RAM)

Start with a smaller model to test, then upgrade based on your hardware capabilities.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“Œ KEYBOARD SHORTCUTS

â€¢ Enter - Send message
â€¢ Shift + Enter - New line in message
â€¢ Tab - Complete slash command (when autocomplete is open)
â€¢ Arrow Up/Down - Navigate autocomplete options
â€¢ Escape - Close autocomplete menu

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

â“ FREQUENTLY ASKED QUESTIONS

Q: Is my data sent to any external servers?
A: No! Sidekick only communicates with the local LLM server you configure. No data is sent to cloud services.

Q: Do I need an internet connection?
A: Only to install the extension. Once set up, Sidekick works entirely offline with your local server.

Q: What models can I use?
A: Any model supported by your LLM server. For Ollama, run "ollama list" to see installed models.

Q: Can I use this with ChatGPT or Claude API?
A: Sidekick is designed for local LLMs, but it technically works with any OpenAI-compatible API. However, using cloud APIs would send your data externally.

Q: Why is the extension asking for broad host permissions?
A: This allows Sidekick to connect to LLM servers on any address - whether localhost, a LAN IP, or a custom domain. Your browsing data is never accessed or transmitted.

Q: Can I use Sidekick on my phone?
A: Sidekick is a Chrome desktop extension. For mobile, check if your LLM server has its own mobile interface.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ’¬ FEEDBACK & SUPPORT

We'd love to hear from you! If you have suggestions, find bugs, or just want to say hi, please leave a review or reach out.

Enjoy your local AI assistant! ğŸ‰
